
# ğŸ¥ Local Ophthalmology Clinical LLM  
**(Ollama + Flask + Docker | CPU & GPU | Windows)**

This project provides a **fully self-hosted, private, clinician-to-clinician AI system** for **ophthalmology EMR interpretation**.

It is designed to behave like a **senior consultant ophthalmologist reviewing another doctorâ€™s notes**, not a patient-facing chatbot.

âœ… No OpenAI  
âœ… No Gemini  
âœ… No external APIs  
âœ… Runs fully offline  
âœ… Full clinical reasoning enabled  

---

## ğŸ¯ Core Capabilities

- ğŸ§  **Doctor-level ophthalmology EMR interpretation**
- ğŸ‘ï¸ Eye-wise (R/OD vs L/OS) clinical reasoning
- ğŸ“„ Accepts **structured EMR JSON**
- ğŸ§¾ Generates **shareable Markdown / PDF reports**
- ğŸ” Identifies:
  - Normal vs abnormal findings
  - Missing but clinically essential data
  - Documentation quality gaps
  - Differential diagnoses (conditional)
  - Suggested next clinical steps

This system is **NOT patient-facing**.

---

## ğŸ§± Technology Stack

| Layer | Technology |
|-----|-----------|
| LLM Runtime | Ollama (Docker) |
| Model | LLaMA 3.1 (8B) |
| API | Flask |
| Export | Markdown / PDF |
| OS | Windows 10 / 11 |
| GPU (optional) | NVIDIA CUDA |

---

## ğŸ“ Project Structure (Actual)

```
MODEL/
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ cpu/
â”‚   â”‚   â””â”€â”€ docker-compose.yml      # CPU-only Ollama
â”‚   â”‚
â”‚   â””â”€â”€ gpu/
â”‚       â””â”€â”€ docker-compose.yml      # GPU-enabled Ollama
â”‚
â”œâ”€â”€ myenv/                           # Python virtual environment
â”‚
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ app.py                      # Flask API
â”‚   â”œâ”€â”€ Requirements.txt
â”‚   â”œâ”€â”€ uploads/                    # Input files (optional)
â”‚   â””â”€â”€ outputs/                    # Generated MD / PDF reports
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ Readme.md

```

---

## ğŸ§  Architecture (Recommended)

```

Windows Host
â”‚
â”œâ”€â”€ Flask API (Python venv)
â”‚   â””â”€â”€ [http://localhost:5000](http://localhost:5000)
â”‚
â””â”€â”€ Docker Desktop
â””â”€â”€ Ollama Runtime
â””â”€â”€ [http://localhost:11434](http://localhost:11434)
â””â”€â”€ llama3.1 model

````

âœ” Flask runs natively on Windows  
âœ” Ollama runs in Docker  
âœ” No Docker rebuilds for Flask  
âœ” Ideal for hospital / EMR environments  

---

## âœ… Prerequisites

- Windows 10 / 11
- Docker Desktop (WSL2 enabled)
- Python 3.11+
- NVIDIA GPU (optional, for GPU mode)

Verify:
```powershell
docker --version
python --version
curl --version
````

---

## ğŸ³ Step 1: Start Ollama (Choose ONE)

### â–¶ CPU Mode

```powershell
cd model/cpu
docker compose up -d
```

### â–¶ GPU Mode (NVIDIA)

```powershell
cd model/gpu
docker compose up -d
```

Verify:

```powershell
docker ps
```

---

## ğŸ“¥ Step 2: Pull LLM Model (One-Time)

```powershell
docker exec -it ollama ollama pull llama3.1
```

Verify:

```powershell
docker exec -it ollama ollama list
```

---

## ğŸ Step 3: Python Virtual Environment

```powershell
cd MODEL
py -3.11 -m venv myenv
myenv\Scripts\Activate.ps1
```

---

## ğŸ“¦ Step 4: Install Python Dependencies

```powershell
cd server
pip install -r Requirements.txt
```

---

## ğŸš€ Step 5: Run Flask API

```powershell
python app.py
```

API runs at:

```
http://localhost:5000
```

---

## ğŸ”— API Endpoints

### âœ… Health Check

```powershell
curl http://localhost:5000/health
```

---

### ğŸ§  Analyze Ophthalmology EMR (JSON)

**Doctor-level clinical interpretation**

```powershell
curl -X POST http://localhost:5000/analyze-examination-json `
  -H "Authorization: Bearer sk-local-key" `
  -H "Content-Type: application/json" `
  --data-binary "@exam.json"
```

Returns:

* Consultant-style interpretation
* Missing data analysis
* Differential diagnoses (conditional)
* Suggested next steps
* Documentation critique

---

### ğŸ“„ Export Shareable Report (MD / PDF)

#### Markdown

```powershell
curl -X POST "http://localhost:5000/export-examination?format=md" `
  -H "Authorization: Bearer sk-local-key" `
  -H "Content-Type: application/json" `
  --data-binary "@exam.json"
```

#### PDF

```powershell
curl -X POST "http://localhost:5000/export-examination?format=pdf" `
  -H "Authorization: Bearer sk-local-key" `
  -H "Content-Type: application/json" `
  --data-binary "@exam.json"
```

Files saved to:

```
server/outputs/
```

---

## ğŸ§  Model Behavior (Important)

The model is explicitly instructed to:
````
âœ” Treat empty fields as **NOT DOCUMENTED**
âœ” Never assume normal findings
âœ” Never invent measurements
âœ” Never correct EMR data
âœ” Separate findings vs limitations
âœ” Think like a real ophthalmologist
````
This avoids unsafe hallucinations.

---

## âš ï¸ Clinical Disclaimer

This system is **for clinician-to-clinician decision support only**.

It does **not** replace:

* Independent medical judgment
* Physical examination
* Diagnostic confirmation

